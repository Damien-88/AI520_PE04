{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecb61a02",
   "metadata": {},
   "source": [
    "Import Libraries and Download Resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36aaeaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\Nikolai\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Libraries\n",
    "import nltk\n",
    "import random\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Movie reviews dataset\n",
    "from nltk.corpus import movie_reviews\n",
    "nltk.download(\"movie_reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c991f4",
   "metadata": {},
   "source": [
    "## Implement Basic Text Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e352a009",
   "metadata": {},
   "source": [
    "Load and Prepare Movie Reviews Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e96e3cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Shuffle Movie Reviews Dataset\n",
    "# Each document is a tuple of (words, category)\n",
    "documents = [(list(movie_reviews.words(fileid)), category)\n",
    "             for category in movie_reviews.categories()\n",
    "             for fileid in movie_reviews.fileids(category)]\n",
    "\n",
    "random.shuffle(documents)\n",
    "\n",
    "# Prepare Data\n",
    "doc_text = [\" \".join(doc) for doc, _ in documents] # Extract and join words\n",
    "doc_labels = [label for _, label in documents] # Extrtact labels\n",
    "\n",
    "# Create DataFrame for easier manipulation\n",
    "review_df = pd.DataFrame({\n",
    "    \"text\": doc_text,\n",
    "    \"label\": doc_labels\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e9099cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Overview:\n",
      "label\n",
      "pos    1000\n",
      "neg    1000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Sample Reviews:\n",
      "                                                text label\n",
      "0  i remember hearing about this film when it fir...   pos\n",
      "1  the keen wisdom of an elderly bank robber , th...   pos\n",
      "2  wild things is a way to steam up an otherwise ...   neg\n",
      "3  star wars : ? episode i -- the phantom menace ...   neg\n",
      "4  weir is well - respected in hollywood for turn...   pos\n"
     ]
    }
   ],
   "source": [
    "# Display Dataset Overview and Sample Reviews\n",
    "print(f\"Dataset Overview:\\n{review_df[\"label\"].value_counts()}\")\n",
    "print(f\"\\nSample Reviews:\\n{review_df.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b59022",
   "metadata": {},
   "source": [
    "Convert Reviews into Suitable ML Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "308c9340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data into Training and Testing Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    review_df[\"text\"], # Extract text for training\n",
    "    review_df[\"label\"], # Extract labels for training\n",
    "    test_size = 0.2, # 20% of data for testing\n",
    "    random_state = 42 # Ensure reproducibility\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67812729",
   "metadata": {},
   "source": [
    "Use CountVectorizer to Convert Text to Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b2c02b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CountVectorizer\n",
    "count_vectorizer = CountVectorizer(max_features = 5000, stop_words = \"english\") # Convert text to features\n",
    "\n",
    "# Fit and transform training data, then transform test data\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train) # Fit and transform training data\n",
    "X_test_counts = count_vectorizer.transform(X_test) # Transform test data using the same vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d565b1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:\n",
      "5000\n",
      "\n",
      "Top features:\n",
      "['000' '10' '100' '11' '12' '13' '13th' '14' '15' '16']\n"
     ]
    }
   ],
   "source": [
    "# Print number of features and top features\n",
    "print(f\"Number of features:\\n{len(count_vectorizer.get_feature_names_out())}\")\n",
    "print(f\"\\nTop features:\\n{count_vectorizer.get_feature_names_out()[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e6380e",
   "metadata": {},
   "source": [
    "Train Naive Bayes Classifier and Evaluate its Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "118d7d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_clf = MultinomialNB() # Initialize Naive Bayes classifier\n",
    "nb_clf.fit(X_train_counts, y_train) # Train the classifier\n",
    "y_pred = nb_clf.predict(X_test_counts) # Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae07f4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.77      0.86      0.81       189\n",
      "         pos       0.86      0.76      0.81       211\n",
      "\n",
      "    accuracy                           0.81       400\n",
      "   macro avg       0.81      0.81      0.81       400\n",
      "weighted avg       0.82      0.81      0.81       400\n",
      "\n",
      "\n",
      "Accuracy: 0.8100\n"
     ]
    }
   ],
   "source": [
    "# Print detailed performance metrics and overall accuracy\n",
    "print(f\"Classification Report: \\n{classification_report(y_test, y_pred)}\")\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd33262",
   "metadata": {},
   "source": [
    "## Understanding CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe53d23d",
   "metadata": {},
   "source": [
    "1. Explain the role of CountVectorizer in text classification. How does it prepare text data for machine learning? \n",
    "- CountVectorizer is a feature extraction tool that transforms raw text documents into a numerical format that machine learning models can understand. It works by first tokenizing the text, breaking it down into individual words. It then counts the frequency of each word within a document, creating a \"bag-of-words\" model. In this model, each document is represented as a numerical vector where the values correspond to the raw counts of each word.\n",
    "\n",
    "2. What is the purpose of train_test_split? Why is it important in machine learning evaluation? \n",
    "\n",
    "- The train_test_split function divides a dataset into separate training and testing subsets. Doing do is essential for evaluating a model's ability to generalize to new \"unseen\" data. If you were to train and test a model on the same data, it might simply memorize the examples. Ultimately leading to overfitting and inaccurate performance metrics. By training the model on one portion of the data and then evaluating its performance on a separate portion, train_test_split provides a more realistic and reliable estimate of how the model will perform in real-world scenarios.\n",
    "\n",
    "3. Analyze the accuracy calculation in the Naive Bayes classifier. What does the score tell us about model performance?\n",
    "- The accuracy score of the Naive Bayes classifier measures the proportion of correctly predicted movie reviews in the test set. For this particular movie review dataset, which is well-balanced with an equal number of positive and negative reviews, accuracy serves as a reliable indicator of overall performance. An accuracy of 81% means the model correctly classified 81% of the test reviews. Because the dataset is balanced, this score suggests that the model is effective at distinguishing between both positive and negative sentiment classes without being biased toward one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64320460",
   "metadata": {},
   "source": [
    "## Advanced Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6beea0",
   "metadata": {},
   "source": [
    "Modify Code to use TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "35ea280b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TF-IDF Vectorizer\n",
    "tf_count_vectorizer = TfidfVectorizer(\n",
    "    max_features = 5000, # Limit to 5000 features\n",
    "    min_df = 5, # Minimum document frequency of 5\n",
    "    max_df = 0.7, # Maximum document frequency of 70%\n",
    "    stop_words = \"english\" # Remove common English stop words\n",
    ")\n",
    "\n",
    "# Fit and transform training data, then transform test data\n",
    "X_train_tfidf = tf_count_vectorizer.fit_transform(X_train) # Fit and transform training data\n",
    "X_test_tfidf = tf_count_vectorizer.transform(X_test) # Transform test data using the same vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0c1d799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features:\n",
      "5000\n",
      "\n",
      "Top features:\n",
      "['000' '10' '100' '101' '11' '12' '13' '13th' '14' '15']\n"
     ]
    }
   ],
   "source": [
    "# Print number of features and top features\n",
    "print(f\"Number of features:\\n{len(tf_count_vectorizer.get_feature_names_out())}\")\n",
    "print(f\"\\nTop features:\\n{tf_count_vectorizer.get_feature_names_out()[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e68817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Naive Bayes Classifier and evaluate its performance\n",
    "nb_clf_tfidf = MultinomialNB() # Initialize Naive Bayes classifier\n",
    "nb_clf_tfidf.fit(X_train_tfidf, y_train) # Train the classifier\n",
    "y_pred_tfidf = nb_clf_tfidf.predict(X_test_tfidf) # Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f5e8e030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.75      0.87      0.81       189\n",
      "         pos       0.86      0.74      0.80       211\n",
      "\n",
      "    accuracy                           0.80       400\n",
      "   macro avg       0.81      0.81      0.80       400\n",
      "weighted avg       0.81      0.80      0.80       400\n",
      "\n",
      "\n",
      "Accuracy: 0.8025\n"
     ]
    }
   ],
   "source": [
    "# Print detailed performance metrics and overall accuracy\n",
    "print(f\"Classification Report: \\n{classification_report(y_test, y_pred_tfidf)}\")\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred_tfidf):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9278c3d2",
   "metadata": {},
   "source": [
    "Compare Accuracy Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef56579",
   "metadata": {},
   "source": [
    "The Naive Bayes classifier achieved slightly higher accuracy using CountVectorizer (81.00%) compared to TfidfVectorizer (80.25%). Both approaches yielded similar macro-averaged F1 scores (0.81 vs 0.80), with CountVectorizer having a slight edge in recall for the positive class. While the difference is small, CountVectorizer showed marginally more balanced performance across both sentiment classes in this binary classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4e07f6",
   "metadata": {},
   "source": [
    "Why One May Perform Better than the Other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f85f20",
   "metadata": {},
   "source": [
    "- CountVectorizer may perform better in this instance because it retains the full weight of frequently occurring words, many of which are strong sentiment indicators. \n",
    "\n",
    "- TfidfVectorizer down-weights common words across the dataset. While this is useful for tasks like topic modeling to highlight unique words, it can inadvertently reduce the impact of these high-frequency sentiment indicators.\n",
    "\n",
    "- Since the Naive Bayes algorithm strongly relies on word frequency to make its classifications, CountVectorizer's direct count-based approach aligns more closely with the model's underlying assumptions. Which leads to slightly better performance in this specific scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0106405",
   "metadata": {},
   "source": [
    "## Alternative Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0a7b05",
   "metadata": {},
   "source": [
    "Implement and compare an SVM classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc96c992",
   "metadata": {},
   "source": [
    "Replace Naive Bayes with LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c21b534",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM with CountVectorizer features\n",
    "svm_clf = LinearSVC(max_iter = 10000) # Initialize SVM classifier\n",
    "svm_clf.fit(X_train_counts, y_train) # Train the classifier\n",
    "y_pred_svm = svm_clf.predict(X_test_counts) # Predict on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca695a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report: \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         neg       0.81      0.85      0.83       189\n",
      "         pos       0.86      0.82      0.84       211\n",
      "\n",
      "    accuracy                           0.83       400\n",
      "   macro avg       0.83      0.84      0.83       400\n",
      "weighted avg       0.84      0.83      0.84       400\n",
      "\n",
      "\n",
      "Accuracy: 0.8350\n"
     ]
    }
   ],
   "source": [
    "# Print detailed performance metrics and overall accuracy\n",
    "print(f\"Classification Report: \\n{classification_report(y_test, y_pred_svm)}\")\n",
    "print(f\"\\nAccuracy: {accuracy_score(y_test, y_pred_svm):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abe1851",
   "metadata": {},
   "source": [
    "Compare Performace Metrics between Naive Bayes and SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80629669",
   "metadata": {},
   "source": [
    "- The SVM classifier achieved an accuracy of 83.5%. Which outperformed the Naive Bayes model’s 81.0%. \n",
    "\n",
    "- The SVM model demonstrated strong performance, achieving a balanced F1-score across both positive and negative sentiment classes.\n",
    "\n",
    "- For negative reviews, the model had an F1-score of 0.83, while for positive reviews, it achieved an F1-score of 0.84. The macro-averaged F1-score of 0.83 further confirms the model's consistent and reliable performance across both classes.\n",
    "\n",
    "- All-in-all, the SVM model proved to be more effective than the Naive Bayes classifier. It was particularly adept at creating a more refined decision boundary, leading to better classification of ambiguous examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88097af7",
   "metadata": {},
   "source": [
    "Discuss Trade-offs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4edbe08",
   "metadata": {},
   "source": [
    "- SVM had better accuracy and slightly better balance between precision and recall. SVMs require more training time and may need careful hyperparameter tuning. For larger datasets, you may need to increase the max_iter parameter to ensure the model converges. \n",
    "\n",
    "- Naive Bayes trains quickly and is easier to interpret. Which is great for quickfire prototyping or with limited resources. Its simplicity and speed can be a significant advantage in many scenarios.\n",
    "\n",
    "- The SVM’s ability to model more complex relationships between features could justify its extra computational cost when higher accuracy is needed.\n",
    "\n",
    "- Naive Bayes is more suited when speed and simplicity are required."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
